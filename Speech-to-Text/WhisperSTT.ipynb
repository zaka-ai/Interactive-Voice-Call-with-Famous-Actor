{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install required packages"
      ],
      "metadata": {
        "id": "ouoftG9GSW7U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUu9M_kHSJOf",
        "outputId": "1a7ddbc1-2d8f-4bb1-e21e-8be9b97e2af7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/800.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m108.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m123.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q openai-whisper\n",
        "!pip install -q gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuGr1FaFQgIc",
        "outputId": "05e68053-a74c-4d3b-f2e1-efd03205c4e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Import required libraries"
      ],
      "metadata": {
        "id": "9a6GBY9PSeI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import gradio as gr\n",
        "import os\n",
        "import tempfile\n",
        "import torch"
      ],
      "metadata": {
        "id": "ggBIw9cjSg01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Create the WhisperSTT class"
      ],
      "metadata": {
        "id": "zZB01TeHZTVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WhisperSTT:\n",
        "    \"\"\"\n",
        "    Speech-to-Text class using OpenAI's Whisper model for Arabic transcription\n",
        "    with automatic text correction using Google's Gemini model when possible.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_size=\"medium\", gemini_api_key=None):\n",
        "        \"\"\"\n",
        "        Initialize the Whisper model for Arabic speech recognition.\n",
        "\n",
        "        Args:\n",
        "            model_size (str): Size of the Whisper model (tiny, base, small, medium, large)\n",
        "            gemini_api_key (str, optional): API key for Google's Gemini model for text correction\n",
        "        \"\"\"\n",
        "        self.model = whisper.load_model(model_size)\n",
        "        self.model_size = model_size\n",
        "        self.gemini_api_key = gemini_api_key\n",
        "        self.gemini_client = None\n",
        "\n",
        "        # Initialize Gemini client if API key is provided\n",
        "        if gemini_api_key:\n",
        "            try:\n",
        "                from google import genai\n",
        "                self.gemini_client = genai.Client(api_key=gemini_api_key)\n",
        "                print(\"Gemini text correction enabled\")\n",
        "            except ImportError:\n",
        "                print(\"Warning: google-generativeai package not installed. Using raw transcription.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Failed to initialize Gemini client: {str(e)}. Using raw transcription.\")\n",
        "\n",
        "    def transcribe(self, audio_path):\n",
        "        \"\"\"\n",
        "        Transcribe Arabic speech to text with automatic text correction when possible.\n",
        "\n",
        "        Args:\n",
        "            audio_path (str): Path to the audio file\n",
        "\n",
        "        Returns:\n",
        "            str: Transcribed text (corrected if Gemini is available, otherwise raw)\n",
        "        \"\"\"\n",
        "        if not os.path.exists(audio_path):\n",
        "            raise FileNotFoundError(f\"Audio file not found: {audio_path}\")\n",
        "\n",
        "        # Transcribe the audio with Arabic language hint\n",
        "        result = self.model.transcribe(audio_path, language=\"ar\")\n",
        "        transcription = result[\"text\"]\n",
        "\n",
        "        # Try to apply text correction if Gemini client is available\n",
        "        if self.gemini_client and self.gemini_api_key:\n",
        "            try:\n",
        "                corrected_text = self._correct_text(transcription)\n",
        "                if corrected_text:\n",
        "                    return corrected_text\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Text correction failed: {str(e)}. Using raw transcription.\")\n",
        "\n",
        "        return transcription\n",
        "\n",
        "    def _correct_text(self, text):\n",
        "        \"\"\"\n",
        "        Correct the transcribed text using Google's Gemini model.\n",
        "\n",
        "        Args:\n",
        "            text (str): Original transcribed text\n",
        "\n",
        "        Returns:\n",
        "            str: Corrected text, or None if correction fails\n",
        "        \"\"\"\n",
        "        try:\n",
        "            from google import genai\n",
        "\n",
        "            prompt = f\"\"\"\n",
        "            صحّح النص التالي المكتوب باللهجة اللبنانية:\n",
        "            - صحّح الأخطاء الإملائية والنحوية فقط.\n",
        "            - إذا كان في جملة استفهامية، ضيف علامة استفهام.\n",
        "            - ما تضيف ولا كلمة زيادة أو شرح.\n",
        "            - رجّع فقط النص المصحَّح، بدون علامات تنصيص أو أي إضافات.\n",
        "            النص: \"{text}\"\n",
        "            \"\"\"\n",
        "\n",
        "            response = self.gemini_client.models.generate_content(\n",
        "            model=\"gemini-2.0-flash\", contents=prompt\n",
        "            )\n",
        "            return response.text.strip()\n",
        "        except Exception as e:\n",
        "            print(f\"Text correction error: {str(e)}\")\n",
        "            return None"
      ],
      "metadata": {
        "id": "RjskCkjLZQnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.Testing"
      ],
      "metadata": {
        "id": "ZbbOdyveZvhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "print(\"Gemini API key successfully loaded from secrets\")\n",
        "\n",
        "# Initialize with Gemini API key\n",
        "stt_model = WhisperSTT(\n",
        "    model_size=\"medium\",\n",
        "    gemini_api_key=GEMINI_API_KEY\n",
        ")\n",
        "\n",
        "# Transcribe with correction\n",
        "transcription = stt_model.transcribe(\"/content/arabic_speech.wav\")\n",
        "print(transcription)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Utm-sVQHThfi",
        "outputId": "1fa6c4b5-06e5-47f3-b794-4e25d07be3fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini API key successfully loaded from secrets\n",
            "Gemini text correction enabled\n",
            "مرحبا فيروز، كيفك؟ جاهزة للمقابلة اليوم؟\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transcribe with correction\n",
        "transcription = stt_model.transcribe(\"/content/arabic_speech2.wav\")\n",
        "print(transcription)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICOkQR8lW4aD",
        "outputId": "a9c9be77-435a-4717-f1bb-249fb16a73ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "أنا ماغي بو غصن، ممثلة لبنانية. عندي ولدان: ريان ويارا. بيحبوني كتير، بتمنى شوفهم عم يكبروا قدامي وعيش معهم لبقية حياتي.\n"
          ]
        }
      ]
    }
  ]
}